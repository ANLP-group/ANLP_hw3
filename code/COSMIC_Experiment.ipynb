{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216572,"status":"ok","timestamp":1699836310113,"user":{"displayName":"Boyi Qian","userId":"18283378653148978720"},"user_tz":300},"id":"8T2jOo96ypiB","outputId":"e72701b0-77d9-4231-c72f-51b485b175bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive\")"],"metadata":{"id":"bgvEHo4QKztc","executionInfo":{"status":"ok","timestamp":1699836841144,"user_tz":300,"elapsed":558,"user":{"displayName":"Boyi Qian","userId":"18283378653148978720"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":161,"status":"ok","timestamp":1699837840117,"user":{"displayName":"Boyi Qian","userId":"18283378653148978720"},"user_tz":300},"id":"FnAJS9vuyuXy","outputId":"211e5131-6524-4b2e-d8a3-e2fd5437e641"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1-3THX-pVEawAfZ6tnf9PtnCAXWOUksLv/COSMIC/erc-training\n"]}],"source":["cd /content/drive/MyDrive/COSMIC/erc-training"]},{"cell_type":"code","source":["import numpy as np, argparse, time, pickle, random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.nn import functional as F\n","from dataloader import IEMOCAPRobertaCometDataset\n","from model import MaskedNLLLoss\n","from commonsense_model import CommonsenseGRUModel\n","from sklearn.metrics import f1_score, accuracy_score\n","import datetime\n","\n","\n","def seed_everything(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","def get_IEMOCAP_loaders(batch_size=32, num_workers=0, pin_memory=False):\n","    trainset = IEMOCAPRobertaCometDataset('train')\n","    validset = IEMOCAPRobertaCometDataset('valid')\n","    testset = IEMOCAPRobertaCometDataset('test')\n","\n","    train_loader = DataLoader(trainset,\n","                              batch_size=batch_size,\n","                              collate_fn=trainset.collate_fn,\n","                              num_workers=num_workers,\n","                              pin_memory=pin_memory)\n","\n","    valid_loader = DataLoader(validset,\n","                              batch_size=batch_size,\n","                              collate_fn=trainset.collate_fn,\n","                              num_workers=num_workers,\n","                              pin_memory=pin_memory)\n","\n","    test_loader = DataLoader(testset,\n","                             batch_size=batch_size,\n","                             collate_fn=testset.collate_fn,\n","                             num_workers=num_workers,\n","                             pin_memory=pin_memory)\n","\n","    return train_loader, valid_loader, test_loader\n","\n","\n","def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n","    losses, preds, labels, masks, losses_sense = [], [], [], [], []\n","    alphas, alphas_f, alphas_b, vids = [], [], [], []\n","    max_sequence_len = []\n","\n","    assert not train or optimizer != None\n","    if train:\n","        model.train()\n","    else:\n","        model.eval()\n","\n","    seed_everything(seed)\n","    for data in dataloader:\n","        if train:\n","            optimizer.zero_grad()\n","\n","        r1, r2, r3, r4, \\\n","            x1, x2, x3, x4, x5, x6, \\\n","            o1, o2, o3, \\\n","            qmask, umask, label = [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n","\n","        log_prob, _, alpha, alpha_f, alpha_b, _ = model(r1, r2, r3, r4, x5, x6, x1, o2, o3, qmask, umask, att2=True)\n","\n","        lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])  # batch*seq_len, n_classes\n","        labels_ = label.view(-1)  # batch*seq_len\n","        loss = loss_function(lp_, labels_, umask)\n","\n","        pred_ = torch.argmax(lp_, 1)  # batch*seq_len\n","        preds.append(pred_.data.cpu().numpy())\n","        labels.append(labels_.data.cpu().numpy())\n","        masks.append(umask.view(-1).cpu().numpy())\n","        losses.append(loss.item() * masks[-1].sum())\n","\n","        if train:\n","            total_loss = loss\n","            total_loss.backward()\n","            if args.tensorboard:\n","                for param in model.named_parameters():\n","                    writer.add_histogram(param[0], param[1].grad, epoch)\n","            optimizer.step()\n","        else:\n","            alphas += alpha\n","            alphas_f += alpha_f\n","            alphas_b += alpha_b\n","            vids += data[-1]\n","\n","    if preds != []:\n","        preds = np.concatenate(preds)\n","        labels = np.concatenate(labels)\n","        masks = np.concatenate(masks)\n","    else:\n","        return float('nan'), float('nan'), float('nan'), [], [], [], float('nan'), []\n","\n","    avg_loss = round(np.sum(losses) / np.sum(masks), 4)\n","    avg_sense_loss = round(np.sum(losses_sense) / np.sum(masks), 4)\n","\n","    avg_accuracy = round(accuracy_score(labels, preds, sample_weight=masks) * 100, 2)\n","    avg_fscore = round(f1_score(labels, preds, sample_weight=masks, average='weighted') * 100, 2)\n","\n","    return avg_loss, avg_accuracy, labels, preds, masks, [avg_fscore], [alphas, alphas_f, alphas_b, vids]\n","\n","\n","def main():\n","\n","    parser = argparse.ArgumentParser()\n","\n","    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n","    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n","    parser.add_argument('--l2', type=float, default=0.0003, metavar='L2', help='L2 regularization weight')\n","    parser.add_argument('--rec-dropout', type=float, default=0.1, metavar='rec_dropout', help='rec_dropout rate')\n","    parser.add_argument('--dropout', type=float, default=0.25, metavar='dropout', help='dropout rate')\n","    parser.add_argument('--batch-size', type=int, default=16, metavar='BS', help='batch size')\n","    parser.add_argument('--epochs', type=int, default=60, metavar='E', help='number of epochs')\n","    parser.add_argument('--class-weight', action='store_true', default=False, help='use class weights')\n","    parser.add_argument('--active-listener', action='store_true', default=False, help='active listener')\n","    parser.add_argument('--attention', default='general2', help='Attention type in context GRU')\n","    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n","    parser.add_argument('--mode1', type=int, default=2, help='Roberta features to use')\n","    parser.add_argument('--seed', type=int, default=100, metavar='seed', help='seed')\n","    parser.add_argument('--norm', type=int, default=3, help='normalization strategy')\n","    parser.add_argument('--residual', action='store_true', default=False, help='use residual connection')\n","\n","    args = parser.parse_args()\n","    print(args)\n","\n","    args.cuda = torch.cuda.is_available() and not args.no_cuda\n","    if args.cuda:\n","        print('Running on GPU')\n","    else:\n","        print('Running on CPU')\n","\n","    if args.tensorboard:\n","        from tensorboardX import SummaryWriter\n","\n","        writer = SummaryWriter()\n","\n","    emo_gru = True\n","    n_classes = 6\n","    cuda = args.cuda\n","    n_epochs = args.epochs\n","    batch_size = args.batch_size\n","\n","    global D_s\n","\n","    D_m = 1024\n","    D_s = 768\n","    D_g = 150\n","    D_p = 150\n","    D_r = 150\n","    D_i = 150\n","    D_h = 100\n","    D_a = 100\n","\n","    D_e = D_p + D_r + D_i\n","\n","    global seed\n","    seed = args.seed\n","    # seed_everything(seed)\n","\n","    model = CommonsenseGRUModel(D_m, D_s, D_g, D_p, D_r, D_i, D_e, D_h, D_a,\n","                                n_classes=n_classes,\n","                                listener_state=args.active_listener,\n","                                context_attention=args.attention,\n","                                dropout_rec=args.rec_dropout,\n","                                dropout=args.dropout,\n","                                emo_gru=emo_gru,\n","                                mode1=args.mode1,\n","                                norm=args.norm,\n","                                residual=args.residual)\n","\n","    print('IEMOCAP COSMIC Model.')\n","\n","    if cuda:\n","        model.cuda()\n","\n","    loss_weights = torch.FloatTensor([1 / 0.086747,\n","                                      1 / 0.144406,\n","                                      1 / 0.227883,\n","                                      1 / 0.160585,\n","                                      1 / 0.127711,\n","                                      1 / 0.252668])\n","\n","    if args.class_weight:\n","        loss_function = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n","    else:\n","        loss_function = MaskedNLLLoss()\n","\n","    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n","\n","    lf = open('logs/cosmic_iemocap_logs.txt', 'a')\n","\n","    train_loader, valid_loader, test_loader = get_IEMOCAP_loaders(batch_size=batch_size,\n","                                                                  num_workers=0)\n","\n","    valid_losses, valid_fscores = [], []\n","    test_fscores, test_losses = [], []\n","    best_loss, best_label, best_pred, best_mask = None, None, None, None\n","    best_val_fscore = float('-inf')\n","    best_model_path = \"best_model.pt\"\n","\n","    for e in range(n_epochs):\n","        start_time = time.time()\n","        train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function, train_loader, e,\n","                                                                              optimizer, True)\n","        valid_loss, valid_acc, _, _, _, valid_fscore, _ = train_or_eval_model(model, loss_function, valid_loader, e)\n","        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model,\n","                                                                                                             loss_function,\n","                                                                                                             test_loader,\n","                                                                                                             e)\n","\n","        valid_losses.append(valid_loss)\n","        valid_fscores.append(valid_fscore)\n","        test_losses.append(test_loss)\n","        test_fscores.append(test_fscore)\n","        # Update to check for best validation F-score\n","        if valid_fscore[0] > best_val_fscore:  # Assuming valid_fscore[0] is the F-score\n","            best_val_fscore = valid_fscore[0]\n","            current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","            best_model_path = f\"model_{current_time}_best_model.pt\"\n","            torch.save(model.state_dict(), best_model_path)\n","            print(f\"Epoch {e + 1}: New best validation F-score. Model saved to {best_model_path}\")\n","\n","        if args.tensorboard:\n","            writer.add_scalar('test: accuracy/loss', test_acc / test_loss, e)\n","            writer.add_scalar('train: accuracy/loss', train_acc / train_loss, e)\n","\n","        x = 'epoch: {}, train_loss: {}, acc: {}, fscore: {}, valid_loss: {}, acc: {}, fscore: {}, test_loss: {}, acc: {}, fscore: {}, time: {} sec'.format(\n","            e + 1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, valid_fscore, test_loss, test_acc,\n","            test_fscore, round(time.time() - start_time, 2))\n","\n","        print(x)\n","        lf.write(x + '\\n')\n","\n","    if args.tensorboard:\n","        writer.close()\n","\n","    valid_fscores = np.array(valid_fscores).transpose()\n","    test_fscores = np.array(test_fscores).transpose()\n","\n","    score1 = test_fscores[0][np.argmin(valid_losses)]\n","    score2 = test_fscores[0][np.argmax(valid_fscores[0])]\n","    scores = [score1, score2]\n","    scores = [str(item) for item in scores]\n","\n","    print('Test Scores: Weighted F1')\n","    print('@Best Valid Loss: {}'.format(score1))\n","    print('@Best Valid F1: {}'.format(score2))\n","\n","    rf = open('results/cosmic_iemocap_results.txt', 'a')\n","    rf.write('\\t'.join(scores) + '\\t' + str(args) + '\\n')\n","    rf.close()\n"],"metadata":{"id":"y6Q9p2OEko9l","executionInfo":{"status":"ok","timestamp":1699837873429,"user_tz":300,"elapsed":320,"user":{"displayName":"Boyi Qian","userId":"18283378653148978720"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"twuxRla6O2vD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HaoKZYx9K7DV"},"outputs":[],"source":["!python train_iemocap.py --active-listener # best model 1"]},{"cell_type":"code","source":["!python train_iemocap.py --active-listener # best model 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oLV-ZTwvvyIP","executionInfo":{"status":"ok","timestamp":1699771857880,"user_tz":300,"elapsed":2014702,"user":{"displayName":"Danyating Shen","userId":"05499255179243420900"}},"outputId":"7ab58cd0-0165-4111-c096-25a8809c14ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(no_cuda=False, lr=0.0001, l2=0.0003, rec_dropout=0.1, dropout=0.25, batch_size=16, epochs=60, class_weight=False, active_listener=True, attention='general2', tensorboard=False, mode1=2, seed=100, norm=3, residual=False)\n","Running on CPU\n","IEMOCAP COSMIC Model.\n","/content/drive/MyDrive/CMU/anlp/hw3/COSMIC/erc-training/dataloader.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  return torch.FloatTensor(self.roberta1[vid]),\\\n","Epoch 1: New best validation F-score. Model saved to best_model.pt\n","epoch: 1, train_loss: 1.7527, acc: 22.8, fscore: [18.67], valid_loss: 1.7267, acc: 15.92, fscore: [4.37], test_loss: 1.7084, acc: 23.66, fscore: [9.05], time: 41.71 sec\n","Epoch 2: New best validation F-score. Model saved to best_model.pt\n","epoch: 2, train_loss: 1.6266, acc: 34.38, fscore: [29.53], valid_loss: 1.6293, acc: 21.95, fscore: [15.45], test_loss: 1.5982, acc: 35.61, fscore: [28.61], time: 41.64 sec\n","Epoch 3: New best validation F-score. Model saved to best_model.pt\n","epoch: 3, train_loss: 1.4845, acc: 44.45, fscore: [38.78], valid_loss: 1.4628, acc: 47.76, fscore: [40.3], test_loss: 1.4345, acc: 46.64, fscore: [39.5], time: 42.25 sec\n","Epoch 4: New best validation F-score. Model saved to best_model.pt\n","epoch: 4, train_loss: 1.3437, acc: 48.52, fscore: [42.11], valid_loss: 1.3006, acc: 50.54, fscore: [41.88], test_loss: 1.2872, acc: 51.51, fscore: [41.9], time: 41.93 sec\n","Epoch 5: New best validation F-score. Model saved to best_model.pt\n","epoch: 5, train_loss: 1.2465, acc: 49.72, fscore: [43.21], valid_loss: 1.2032, acc: 55.49, fscore: [50.4], test_loss: 1.1714, acc: 57.61, fscore: [53.01], time: 42.62 sec\n","Epoch 6: New best validation F-score. Model saved to best_model.pt\n","epoch: 6, train_loss: 1.167, acc: 52.82, fscore: [48.47], valid_loss: 1.138, acc: 55.64, fscore: [50.44], test_loss: 1.0913, acc: 60.32, fscore: [56.74], time: 42.12 sec\n","Epoch 7: New best validation F-score. Model saved to best_model.pt\n","epoch: 7, train_loss: 1.1073, acc: 54.06, fscore: [51.42], valid_loss: 1.1201, acc: 54.1, fscore: [50.72], test_loss: 1.0487, acc: 61.55, fscore: [59.11], time: 42.96 sec\n","epoch: 8, train_loss: 1.0715, acc: 55.49, fscore: [53.55], valid_loss: 1.112, acc: 52.55, fscore: [49.79], test_loss: 1.0271, acc: 61.61, fscore: [59.29], time: 42.72 sec\n","Epoch 9: New best validation F-score. Model saved to best_model.pt\n","epoch: 9, train_loss: 1.032, acc: 57.41, fscore: [56.06], valid_loss: 1.0664, acc: 56.88, fscore: [52.75], test_loss: 0.9845, acc: 62.66, fscore: [60.4], time: 42.96 sec\n","epoch: 10, train_loss: 1.0011, acc: 59.19, fscore: [57.64], valid_loss: 1.0546, acc: 56.26, fscore: [52.43], test_loss: 0.9684, acc: 62.54, fscore: [60.68], time: 42.24 sec\n","epoch: 11, train_loss: 0.9819, acc: 59.91, fscore: [58.9], valid_loss: 1.0409, acc: 55.64, fscore: [52.36], test_loss: 0.9548, acc: 63.15, fscore: [61.7], time: 42.0 sec\n","epoch: 12, train_loss: 0.9698, acc: 60.24, fscore: [59.43], valid_loss: 1.0451, acc: 54.1, fscore: [50.72], test_loss: 0.966, acc: 62.42, fscore: [60.62], time: 42.18 sec\n","Epoch 13: New best validation F-score. Model saved to best_model.pt\n","epoch: 13, train_loss: 0.9407, acc: 61.69, fscore: [60.67], valid_loss: 1.0084, acc: 57.03, fscore: [54.53], test_loss: 0.9517, acc: 61.43, fscore: [60.13], time: 42.26 sec\n","epoch: 14, train_loss: 0.916, acc: 63.35, fscore: [62.8], valid_loss: 1.004, acc: 56.57, fscore: [52.96], test_loss: 0.9402, acc: 62.48, fscore: [61.48], time: 42.6 sec\n","Epoch 15: New best validation F-score. Model saved to best_model.pt\n","epoch: 15, train_loss: 0.9101, acc: 63.08, fscore: [62.24], valid_loss: 0.9795, acc: 58.58, fscore: [55.88], test_loss: 0.9366, acc: 62.78, fscore: [60.96], time: 42.41 sec\n","Epoch 16: New best validation F-score. Model saved to best_model.pt\n","epoch: 16, train_loss: 0.8637, acc: 65.1, fscore: [64.45], valid_loss: 0.983, acc: 59.35, fscore: [57.11], test_loss: 0.9473, acc: 61.37, fscore: [60.15], time: 42.38 sec\n","Epoch 17: New best validation F-score. Model saved to best_model.pt\n","epoch: 17, train_loss: 0.8393, acc: 66.78, fscore: [66.25], valid_loss: 0.9808, acc: 59.66, fscore: [58.49], test_loss: 0.9319, acc: 61.68, fscore: [61.77], time: 42.34 sec\n","epoch: 18, train_loss: 0.8073, acc: 68.47, fscore: [68.07], valid_loss: 0.9577, acc: 59.35, fscore: [57.32], test_loss: 0.9303, acc: 60.94, fscore: [60.0], time: 41.94 sec\n","Epoch 19: New best validation F-score. Model saved to best_model.pt\n","epoch: 19, train_loss: 0.7851, acc: 69.11, fscore: [68.59], valid_loss: 0.9353, acc: 60.12, fscore: [58.51], test_loss: 0.9266, acc: 61.8, fscore: [61.55], time: 41.89 sec\n","Epoch 20: New best validation F-score. Model saved to best_model.pt\n","epoch: 20, train_loss: 0.761, acc: 70.39, fscore: [69.96], valid_loss: 0.9488, acc: 61.67, fscore: [60.7], test_loss: 0.9298, acc: 61.74, fscore: [61.66], time: 42.31 sec\n","epoch: 21, train_loss: 0.7401, acc: 71.16, fscore: [70.64], valid_loss: 0.9703, acc: 58.73, fscore: [56.58], test_loss: 0.9116, acc: 61.92, fscore: [61.01], time: 42.27 sec\n","epoch: 22, train_loss: 0.7161, acc: 72.17, fscore: [71.71], valid_loss: 0.9555, acc: 58.42, fscore: [57.25], test_loss: 0.9236, acc: 62.35, fscore: [62.52], time: 42.11 sec\n","epoch: 23, train_loss: 0.7183, acc: 71.88, fscore: [71.55], valid_loss: 0.9555, acc: 57.65, fscore: [55.6], test_loss: 0.9548, acc: 61.12, fscore: [61.0], time: 42.15 sec\n","epoch: 24, train_loss: 0.7559, acc: 70.33, fscore: [70.0], valid_loss: 0.9816, acc: 58.27, fscore: [57.35], test_loss: 0.9561, acc: 60.38, fscore: [60.26], time: 44.56 sec\n","epoch: 25, train_loss: 0.7882, acc: 68.78, fscore: [68.4], valid_loss: 0.975, acc: 57.81, fscore: [55.78], test_loss: 0.9323, acc: 62.11, fscore: [61.43], time: 44.32 sec\n","epoch: 26, train_loss: 0.7524, acc: 69.24, fscore: [68.61], valid_loss: 1.0562, acc: 56.57, fscore: [54.55], test_loss: 0.9921, acc: 60.07, fscore: [59.31], time: 42.36 sec\n","epoch: 27, train_loss: 0.7379, acc: 70.85, fscore: [70.49], valid_loss: 0.9398, acc: 58.89, fscore: [58.27], test_loss: 0.9208, acc: 63.28, fscore: [63.55], time: 42.21 sec\n","Epoch 28: New best validation F-score. Model saved to best_model.pt\n","epoch: 28, train_loss: 0.6735, acc: 73.85, fscore: [73.63], valid_loss: 0.9227, acc: 61.67, fscore: [61.44], test_loss: 0.906, acc: 63.59, fscore: [63.57], time: 42.23 sec\n","epoch: 29, train_loss: 0.6541, acc: 75.05, fscore: [74.78], valid_loss: 0.9441, acc: 56.57, fscore: [56.44], test_loss: 0.9388, acc: 62.05, fscore: [62.08], time: 43.23 sec\n","epoch: 30, train_loss: 0.6256, acc: 75.98, fscore: [75.67], valid_loss: 1.0187, acc: 57.81, fscore: [56.23], test_loss: 0.9505, acc: 62.54, fscore: [62.05], time: 42.12 sec\n","epoch: 31, train_loss: 0.6199, acc: 76.06, fscore: [75.79], valid_loss: 1.0869, acc: 56.41, fscore: [56.59], test_loss: 1.0011, acc: 59.83, fscore: [59.27], time: 42.29 sec\n","epoch: 32, train_loss: 0.6273, acc: 75.65, fscore: [75.57], valid_loss: 0.9485, acc: 58.42, fscore: [57.23], test_loss: 0.9146, acc: 64.39, fscore: [63.9], time: 42.11 sec\n","epoch: 33, train_loss: 0.5931, acc: 77.59, fscore: [77.21], valid_loss: 0.9663, acc: 60.59, fscore: [60.9], test_loss: 0.9363, acc: 63.4, fscore: [63.47], time: 41.83 sec\n","epoch: 34, train_loss: 0.5862, acc: 77.8, fscore: [77.68], valid_loss: 0.9433, acc: 60.59, fscore: [60.71], test_loss: 0.9772, acc: 63.4, fscore: [63.34], time: 42.59 sec\n","epoch: 35, train_loss: 0.5896, acc: 77.01, fscore: [76.75], valid_loss: 0.9908, acc: 57.81, fscore: [58.11], test_loss: 0.9931, acc: 63.03, fscore: [63.12], time: 42.03 sec\n","epoch: 36, train_loss: 0.5777, acc: 78.17, fscore: [78.02], valid_loss: 1.1219, acc: 54.25, fscore: [53.06], test_loss: 0.9981, acc: 63.15, fscore: [63.01], time: 42.23 sec\n","epoch: 37, train_loss: 0.5694, acc: 78.5, fscore: [78.39], valid_loss: 1.0139, acc: 58.58, fscore: [58.96], test_loss: 0.9516, acc: 64.39, fscore: [64.28], time: 42.18 sec\n","epoch: 38, train_loss: 0.5349, acc: 79.72, fscore: [79.55], valid_loss: 0.9518, acc: 60.9, fscore: [61.25], test_loss: 0.9627, acc: 64.26, fscore: [64.02], time: 42.34 sec\n","epoch: 39, train_loss: 0.5063, acc: 81.62, fscore: [81.44], valid_loss: 1.0667, acc: 57.96, fscore: [56.69], test_loss: 0.9615, acc: 64.26, fscore: [64.34], time: 42.14 sec\n","epoch: 40, train_loss: 0.4934, acc: 82.28, fscore: [82.14], valid_loss: 1.0748, acc: 57.5, fscore: [57.88], test_loss: 0.9675, acc: 63.22, fscore: [63.17], time: 42.06 sec\n","epoch: 41, train_loss: 0.4896, acc: 81.87, fscore: [81.79], valid_loss: 0.9586, acc: 60.74, fscore: [60.6], test_loss: 0.9961, acc: 64.63, fscore: [64.52], time: 42.42 sec\n","epoch: 42, train_loss: 0.4676, acc: 82.82, fscore: [82.67], valid_loss: 1.0809, acc: 57.65, fscore: [57.86], test_loss: 0.9848, acc: 65.06, fscore: [65.21], time: 42.78 sec\n","epoch: 43, train_loss: 0.4401, acc: 84.99, fscore: [84.92], valid_loss: 1.0858, acc: 57.65, fscore: [57.86], test_loss: 0.958, acc: 64.94, fscore: [64.94], time: 42.18 sec\n","epoch: 44, train_loss: 0.4195, acc: 85.47, fscore: [85.4], valid_loss: 1.0663, acc: 58.27, fscore: [58.61], test_loss: 1.0062, acc: 63.59, fscore: [63.4], time: 41.88 sec\n","epoch: 45, train_loss: 0.4055, acc: 86.25, fscore: [86.14], valid_loss: 1.0734, acc: 57.96, fscore: [57.14], test_loss: 0.9663, acc: 66.17, fscore: [66.25], time: 42.01 sec\n","epoch: 46, train_loss: 0.3878, acc: 87.16, fscore: [87.09], valid_loss: 1.1192, acc: 56.57, fscore: [56.87], test_loss: 1.0258, acc: 64.45, fscore: [64.27], time: 42.67 sec\n","epoch: 47, train_loss: 0.3746, acc: 87.82, fscore: [87.77], valid_loss: 1.0925, acc: 57.5, fscore: [57.58], test_loss: 1.0521, acc: 62.97, fscore: [62.81], time: 42.29 sec\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/CMU/anlp/hw3/COSMIC/erc-training/train_iemocap.py\", line 211, in <module>\n","    train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function, train_loader, e,\n","  File \"/content/drive/MyDrive/CMU/anlp/hw3/COSMIC/erc-training/train_iemocap.py\", line 70, in train_or_eval_model\n","    log_prob, _, alpha, alpha_f, alpha_b, _ = model(r1, r2, r3, r4, x5, x6, x1, o2, o3, qmask, umask, att2=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/CMU/anlp/hw3/COSMIC/erc-training/commonsense_model.py\", line 315, in forward\n","    emotions_b, alpha_b = self.cs_rnn_r(rev_r, rev_x1, rev_x2, rev_x3, rev_o1, rev_o2, rev_qmask)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/CMU/anlp/hw3/COSMIC/erc-training/commonsense_model.py\", line 189, in forward\n","    g_, q_, r_, i_, e_, alpha_ = self.dialogue_cell(u_, x1_, x2_, x3_, o1_, o2_, \n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/CMU/anlp/hw3/COSMIC/erc-training/commonsense_model.py\", line 141, in forward\n","    e_ = self.e_cell(es_, e0)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\", line 1439, in forward\n","    ret = _VF.gru_cell(\n","KeyboardInterrupt\n"]}]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}